{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Michaelis-Menten kinetics dynamics",
   "id": "606586832fe3295a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The Michaelis-Menten kinetics is given by the following system of (coupled) ordinary differential equations (ODEs):",
   "id": "fa73b03aeea0e5f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\dot{x}^{(\\nu)}_i = -x^{(\\nu)}_i + \\sum_j w_{i,j} \\cdot \\frac{x^{(\\nu)}_j}{1 + x^{(\\nu)}_j}, \\quad i, j \\in \\{1, \\dots, N\\}\n",
    "$$"
   ],
   "id": "2a54afd56cd70a3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TODOs\n",
    "- Add a 'B' coefficient to the sum term (i.e. $-Bx^{(\\nu)}_i + \\sum_j w_{i,j} \\cdot \\frac{x^{(\\nu)}_j}{1 + x^{(\\nu)}_j}, \\quad i, j \\in \\{1, \\dots, N\\}$)\n",
    "- \"Colonial expansion\" - Make the defects uneven (e.g. base_model W $->$ 10 Ws with p = 0.2 and 40 Ws with p = 0.8)\n",
    "- \"Colonial expansion\" should decrease GCL (higher p $->$ lower GCL)"
   ],
   "id": "93d7d29aa47e367d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:03:51.622207Z",
     "start_time": "2024-09-26T12:03:51.608648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.cm as cm\n",
    "import random"
   ],
   "id": "d4c1b3b541e35701",
   "outputs": [],
   "execution_count": 355
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\dot{x}^{(\\nu)}_i = -Bx^{(\\nu)}_i + \\sum_j w_{i,j} \\cdot \\frac{x^{(\\nu)}_j}{1 + x^{(\\nu)}_j}, \\quad i, j \\in \\{1, \\dots, N\\}\n",
    "$$"
   ],
   "id": "dca66a22852c4aa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:03:51.736708Z",
     "start_time": "2024-09-26T12:03:51.719618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up the system of ODEs\n",
    "def ode_system(t, x, W, B):\n",
    "    N = len(x)  # Get the number of elements in the state vector\n",
    "    derivative_vector = np.zeros(N)  # Initialize the derivative vector (set zeros as default)\n",
    "    for i in range(N):  # Loop over each element in the state vector\n",
    "        # Compute the sum term for the i-th element according to the formula given in the paper\n",
    "        sum_term = sum(W[i, j] * (x[j] / (1 + x[j])) for j in range(N) if j != i)  # list comprehension\n",
    "        derivative_vector[i] = -B*x[i] + sum_term\n",
    "    return derivative_vector"
   ],
   "id": "1cf2934855a85579",
   "outputs": [],
   "execution_count": 356
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "N represents # of genes in each cell  \n",
    "num_of_cells (M) represents # of cells as well as # of defects matrices (defects of weight matrix w)\n",
    "p represents the probability of the base weight matrix to defect (non-zero elements probability to change)  \n",
    "A cohort is a group of cells with similar properties (in our case identical p value and the same base_model weight matrix)  \n",
    "A cell is a network of genes  \n",
    "A gene is a node in the network with its connection's represented by the relevant weight matrix W  \n",
    "We have <num_of_cohorts> cohorts, in each cohort we have <num_of_cells> (previously called M) cells, all cells in this cohort have evolved according to their corresponding weight matrix"
   ],
   "id": "acc35f756b17c6c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:03:51.748911Z",
     "start_time": "2024-09-26T12:03:51.736708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize parameters\n",
    "N = 50  # Number of genes in a cell\n",
    "num_of_cells = 10#100  # (M) Number of cells in each cohort = # of defects matrices\n",
    "avg_deg = 3 # Average # of non-zero elements in the matrix W\n",
    "\n",
    "decide = lambda x: random.random() < x\n",
    "q = 1  # Affects the range of the random numbers generated\n",
    "probability_spacing = 0.25  #0.05\n",
    "num_of_cohorts = int(1 / probability_spacing) + 1\n",
    "p = np.linspace(0, 1, num_of_cohorts)  # probability to create a defect\n",
    "W = [[np.zeros((N, N)) for _ in range(num_of_cells)] for ___ in range(\n",
    "    num_of_cohorts)]  # Initialize the weight matrix with zeros. W[cohort index][defect index / cell index][(row, column)] Perhaps there is a simpler more elegant way to do this with smaller dimension arrays.\n",
    "\n",
    "# I.C is an array where each element represents a cohort with different p and each cohort has an array of cells and each cell has a vector of initial conditions for each gene in the cell. initial_conditions[cohort index][defect index / cell index][gene index]\n",
    "initial_conditions = [[np.random.rand(N) for _ in range(num_of_cells)] for __ in range(num_of_cohorts)]\n",
    "\n",
    "num_of_time_stamps = 1000\n",
    "t_final = 20\n",
    "t = np.linspace(0, t_final, num_of_time_stamps)"
   ],
   "id": "5a8f0c6ffd2b53d",
   "outputs": [],
   "execution_count": 357
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:03:51.876671Z",
     "start_time": "2024-09-26T12:03:51.751945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for cohort_index in range(num_of_cohorts):\n",
    "    # For each cohort (group of <num_of_cells> cells), create a system of <num_of_cells> w defect matrices\n",
    "\n",
    "    # Set the base weight matrix for the current cohort\n",
    "    # TODO: this is weird since I defined avg_deg as the average degree but I use it as a probability distribution (actual average might be different) maybe change that in the future\n",
    "    # TODO: try to make the base model the same for all cohorts later\n",
    "    base_model = np.array([[np.random.uniform(0, 2 * q) if (gene_i != gene_j and decide(avg_deg / (N - 1))) else 0\n",
    "                            for gene_j in range(N)] for gene_i in range(N)])\n",
    "\n",
    "    # TODO: the first iteration is useless since p[0] = 0 by definition and its a simple copy of base_model. Maybe change that in the future\n",
    "    # Create defects of the weight matrix\n",
    "    # TODO: Too many indentations, maybe refactor this code\n",
    "    for defect_index in range(num_of_cells):\n",
    "        for gene_i in range(N):\n",
    "            for gene_j in range(N):\n",
    "                if base_model[gene_i, gene_j] != 0:\n",
    "                    if decide(p[cohort_index]):\n",
    "                        # What if, by chance np.random.uniform(0, 2 * q) is the same as base_model[gene_i, gene_j] or if it is 0? these are edge cases that are rare and therefore not handled\n",
    "                        W[cohort_index][defect_index][gene_i, gene_j] = np.random.uniform(0, 2 * q)\n",
    "                    else:\n",
    "                        W[cohort_index][defect_index][gene_i, gene_j] = base_model[gene_i, gene_j]\n",
    "                else:\n",
    "                    W[cohort_index][defect_index][gene_i, gene_j] = 0"
   ],
   "id": "c21a3d0152441963",
   "outputs": [],
   "execution_count": 358
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "..........................",
   "id": "cc7dc3239be3f87e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:03:51.886621Z",
     "start_time": "2024-09-26T12:03:51.876671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(W)) # num_of_cohorts. W is an array that each element represents a cohort with different p\n",
    "print(len(W[num_of_cohorts - 1])) # num_of_cells. An element in W is an array of 50 weight matrices (each corresponds to a cell)\n",
    "print(len(W[num_of_cohorts - 1][num_of_cells - 1])) # N (num of genes in each cell). Each weight matrix is an NxN matrix\n",
    "print(W[num_of_cohorts - 1][num_of_cells - 1][(N-1, N-1)]) # A connection between the last gene with itself in the last cell in the last cohort (should be 0)"
   ],
   "id": "b13e35cd72159e0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "50\n",
      "0.0\n"
     ]
    }
   ],
   "execution_count": 359
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "..........................",
   "id": "b3448fe2760e70cc"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-26T12:03:51.886621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = [[np.zeros((num_of_time_stamps, N)) for _ in range(num_of_cells)] for __ in range(num_of_cohorts)] # Initialize the results array\n",
    "# results[cohort index][defect index][(row, column)] - rows represents timestamps, columns represent gene index\n",
    "\n",
    "# Solve the ODEs for each cell in each cohort (num_of_cells X num_of_cohorts systems of ODE's (each system has N (num_of_genes) ode's)) and the corresponding initial conditions for the cells\n",
    "for cohort_index in range(num_of_cohorts):\n",
    "    for defect_index in range(num_of_cells):\n",
    "        B = 1\n",
    "        results[cohort_index][defect_index] = solve_ivp(ode_system, [t[0], t[-1]], initial_conditions[cohort_index][defect_index], args=(W[cohort_index][defect_index], B), t_eval=t)[\"y\"].T # Transpose the solution so that rows = timestamps and columns = gene index"
   ],
   "id": "9d20ca39a68139d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "steady_state = np.zeros((num_of_cohorts, N, num_of_cells))\n",
    "# steady_state[cohort index, gene index, cell index]\n",
    "\n",
    "for cohort_j in range(num_of_cohorts):\n",
    "    for cell_index in range(num_of_cells):\n",
    "        steady_state[cohort_j, :, cell_index] = results[cohort_j][cell_index][-1]  # Get the last row (final time) of the cell (the steady state)"
   ],
   "id": "cdfecd3bea174ecb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Change 5 random genes in each cell in each cohort to 0\n",
    "num_of_zero_genes = 5\n",
    "for cohort_index in range(num_of_cohorts):\n",
    "    for cell_index in range(num_of_cells):\n",
    "        for _ in range(num_of_zero_genes):\n",
    "            gene_index = random.randint(0, N - 1)\n",
    "            steady_state[cohort_index, gene_index, cell_index] = 0"
   ],
   "id": "398943205b1854b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate Negative Spearman Correlation",
   "id": "745eaea99921bbed"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "negative_spearman_steady_state = [(1 - (spearmanr(steady_state[i])[0])) for i in range(num_of_cohorts)]\n",
    "# spearman_steady_state[cohort index, cell_i, cell_j]"
   ],
   "id": "1b0ccef8f858cbe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# get the off diagonal elements and graph a distribution of them:\n",
    "off_diagonal_elements = [[] for _ in range(num_of_cohorts)]\n",
    "# off_diagonal_elements[cohort index][off-diagonal element index]. The off-diagonal element index doesn't really have a meaning it is just a way to store these off-diagonal elements.\n",
    "\n",
    "for cohort_index in range(num_of_cohorts):\n",
    "    for cell_i in range(num_of_cells):\n",
    "        for cell_j in range(num_of_cells):\n",
    "            if cell_j > cell_i:\n",
    "                off_diagonal_elements[cohort_index].append(negative_spearman_steady_state[cohort_index][cell_i][cell_j])\n",
    "\n",
    "# Round to 2 decimal places to make the graph look normal\n",
    "off_diagonal_elements = [np.round(element, decimals=2) for element in off_diagonal_elements]"
   ],
   "id": "95410bdcccc22f27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot the off-diagonal elements distribution\n",
    "for cohort_index in range(num_of_cohorts):\n",
    "    plt.hist(off_diagonal_elements[cohort_index], alpha=0.5, label='cohort p = {:.2f}'.format(p[cohort_index]))\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Off-diagonal Elements Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "id": "f7499b83dcb79885",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "np.shape(negative_spearman_steady_state)",
   "id": "568514aa82082690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for x in range(len(negative_spearman_steady_state)):\n",
    "    scaler = StandardScaler()\n",
    "    standardized_data = scaler.fit_transform(negative_spearman_steady_state[x])\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(standardized_data)\n",
    "    print(\"Variance ratio:\", pca.explained_variance_ratio_)\n",
    "    print(\"PCA result:\\n\", pca_result)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(p)))\n",
    "    for i in range(num_of_cohorts):\n",
    "        plt.scatter(pca_result[i, 0], pca_result[i, 1], color=colors[i], label=f'p = {p[i]:.2f}')\n"
   ],
   "id": "7a1bff750a70426b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Use dimensionality reduction to reduce the spearman matrix by using PCA\n",
    "\n",
    "# Prepare the data\n",
    "# negative_spearman_steady_state is a list of Negative Spearman correlation matrices (matrix for each cohort)\n",
    "# Flatten the Spearman correlation matrices for PCA (e.g. [[1,2], [3,4]] --> [[1, 2, 3, 4]])\n",
    "flattened_spearman = [matrix.flatten() for matrix in negative_spearman_steady_state]\n",
    "\n",
    "# Scale the data - this will take care of the normalization step I mentioned above \n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(flattened_spearman)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(standardized_data)\n",
    "\n",
    "# [variance of PC1, variance of PC2]\n",
    "print(\"Variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the PCA result [PC1, PC2]\n",
    "print(\"PCA result:\\n\", pca_result)"
   ],
   "id": "55ac5dbe11f704f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot the PCA results with different colors for each point\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(p)))\n",
    "for i in range(num_of_cohorts):\n",
    "    plt.scatter(pca_result[i, 0], pca_result[i, 1], color=colors[i], label=f'p = {p[i]:.2f}')\n",
    "\n",
    "plt.title('PCA of Spearman Correlation Matrices')\n",
    "plt.xlabel(\n",
    "    'Principal Component 1 (accounts for {:.2f}% of the variation)'.format(pca.explained_variance_ratio_[0] * 100))\n",
    "plt.ylabel(\n",
    "    'Principal Component 2 (accounts for {:.2f}% of the variation)'.format(pca.explained_variance_ratio_[1] * 100))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "5ca328ee9680fdc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### PCA\n",
    "check out this video: https://youtu.be/FgakZw6K1QQ\n",
    "\n",
    "##### PC1\n",
    "- Calculate the average measurement for gene 1, 2, 3, ..., N - get the \"center\" of the data\n",
    "- Recalculate the data relative to center (the center is the origin).\n",
    "- Find the best fit linear line that passes through the origin for the data by choosing the line that maximizes the sum of the distances of the points projected by each one of the points on to the line - this line is PC1.\n",
    "- Normalize the line to unit length it is the eigen vector (also called singular vector) of PC1. the proportion of each gene are called \"loading scores\" (kind of like tan(theta) = y/x in 2d). The average of the sum of squared distances (of the points projected on to the PC1 line) is the eigen value for PC1. The square root of the sum of the squared distances is called the singular value for PC1\n",
    "\n",
    "##### PC2\n",
    "- The line that is perpendicular to PC1 (m_pc1 * m_pc2 = -1 for 2d, the number of PC's will be the MIN(variables (genes), samples(cells))) This line is the eigen vector for PC2. The loading scores are the projection of the normalized line on to the gene1 axis (x) and the projection of the normalized line on to the gene2 axis (y). The average of the sum of squared distances (of the points projected on to the PC2 line) is the eigen value for PC2.\n",
    "\n",
    "PC1 accounts for eigen_value_pc1/(eigen_value_pc1 + eigen_value_pc2) of the total variation\n",
    "PC2 accounts for eigen_value_pc2 / (eigen_value_pc1 + eigen_value_pc2) of the total variation\n",
    "\n",
    "We may have many PCi's, but after we calculated the principal components we simply project them to the 2 most dominant pc's (the ones with the highest eigen value i.e. highest variation) NOTE: this will only be good if they account for most of the variation.\n",
    "Use a scree plot for the PCi's"
   ],
   "id": "b5224a9b12e7779d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# from gcl_library import gcl\n",
    "# \n",
    "# print(np.shape(actual_steady_state[0]))\n",
    "# aaa = gcl(actual_steady_state[0], 200)\n",
    "# aaa\n",
    "# print(aaa)"
   ],
   "id": "248f7eef87cea771",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Exercise from a site I found - irrelevant delete it later\n",
    "# import pandas as pd\n",
    "# \n",
    "# url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "# \n",
    "# # load dataset into Pandas DataFrame\n",
    "# df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])"
   ],
   "id": "8b3a76c308932c0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot the results\n",
    "for cohort_index in range(num_of_cohorts):\n",
    "    for defect_index in range(num_of_cells):\n",
    "        for gene in range(N):\n",
    "            plt.plot(t, results[cohort_index][defect_index][:, gene])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('x value')\n",
    "plt.title('Michaelis-Menten kinetics')\n",
    "plt.show()"
   ],
   "id": "e448adc2e13f3d53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ---------",
   "id": "5c9b0f93c359b538"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# # initial state\n",
    "# results[0][0, :]"
   ],
   "id": "e1085715946c36d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# final_state = [np.random.rand(N) for _ in range(N)]\n",
    "# # final state\n",
    "# for m in range(M):\n",
    "#     final_state[m] = results[m][-1, :]"
   ],
   "id": "fb9362171baed38e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# # Calculate the variance of the final state\n",
    "# final_state_variance = np.var(final_state)\n",
    "# print(final_state_variance)"
   ],
   "id": "7f49274c7e8869f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# W2_variance = np.var(W_strong_influence)\n",
    "# print(W_strong_influence)"
   ],
   "id": "ba5fca2bd453048",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the final state vector to a csv file\n",
    "# np.savetxt('final_state.csv', final_state, delimiter=',')"
   ],
   "id": "1bfc9999174e3e02",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
