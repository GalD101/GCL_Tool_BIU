{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Michaelis-Menten kinetics dynamics",
   "id": "606586832fe3295a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The Michaelis-Menten kinetics is given by the following system of (coupled) ordinary differential equations (ODEs):",
   "id": "fa73b03aeea0e5f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\dot{x}^{(\\nu)}_i = -x^{(\\nu)}_i + \\sum_j w_{i,j} \\cdot \\frac{x^{(\\nu)}_j}{1 + x^{(\\nu)}_j}, \\quad i, j \\in \\{1, \\dots, N\\}\n",
    "$$"
   ],
   "id": "2a54afd56cd70a3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TODOs\n",
    "- create many cohorts with uniform p such that every cohort has a different p but an identical \"base model\" weight matrix (p $\\in$ [0.05, 0.1, 0.15, ..., 1]) ✅\n",
    "- run \"negative spearman correlation\" (it's a measure, just like Euclidean distance) on the results ✅\n",
    "- Dimensionality reduction (PCA) - look for libraries that can perform dimensionality reduction. assert a \"cloud like\" distribution\n",
    "- \"Colonial expansion\" - Make the defects uneven (e.g. base_model W $->$ 10 Ws with p = 0.2 and 40 Ws with p = 0.8)\n",
    "- \"Colonial expansion\" should decrease GCL (higher p $->$ lower GCL)"
   ],
   "id": "6ed72975631079d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T10:49:49.199783Z",
     "start_time": "2024-09-20T10:49:46.528964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint  # Import the odeint function from scipy for solving ODEs\n",
    "from scipy.integrate import solve_ivp  # Import the solve_ivp function from scipy for solving ODEs\n",
    "from scipy.stats import spearmanr\n",
    "import random"
   ],
   "id": "920c9329959f5ef1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T10:49:49.213342Z",
     "start_time": "2024-09-20T10:49:49.199783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up the system of ODEs\n",
    "def ode_system(x, t, W):  # t must be passed even if not used\n",
    "    N = len(x)  # Get the number of elements in the state vector\n",
    "    derivative_vector = np.zeros(N)  # Initialize the derivative vector (set zeros as default)\n",
    "    for i in range(N):  # Loop over each element in the state vector\n",
    "        # Compute the sum term for the i-th element according to the formula given in the paper\n",
    "        sum_term = sum(W[i, j] * (x[j] / (1 + x[j])) for j in range(N) if j != i)  # list comprehension\n",
    "        derivative_vector[i] = -x[i] + sum_term\n",
    "    return derivative_vector"
   ],
   "id": "616957f5a27865b2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T10:49:49.244857Z",
     "start_time": "2024-09-20T10:49:49.214854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize parameters\n",
    "N = 100#50  #100  # Number of genes in a cell\n",
    "M = 10#25 #25  #50  # Number of \"defects\" of the matrix W\n",
    "avg_deg = 3\n",
    "\n",
    "initial_conditions = [np.random.rand(N) for _ in range(N)]\n",
    "\n",
    "dist = lambda x: random.random() < x\n",
    "q = 1  # Affects the range of the random numbers generated\n",
    "probability_spacing = 0.1  #0.05\n",
    "num_of_cohorts = int(1 / probability_spacing) + 1\n",
    "p = np.linspace(0, 1, num_of_cohorts) # probability to create a defect\n",
    "W = [[np.zeros((N, N)) for _ in range(M)] for __ in range(\n",
    "    num_of_cohorts)]  # Initialize the weight matrix with zeros; W[cohort index][defect index][(row, column)] TODO: Perhaps there is a simpler more elegant way to do this with smaller dimension arrays.\n",
    "\n",
    "num_of_time_stamps = 100\n",
    "t_final = 10\n",
    "t = np.linspace(0, t_final, num_of_time_stamps)\n",
    "\n",
    "# TODO: Figure out how to make the odeint function stop when the system reaches a steady state or after a certain time period"
   ],
   "id": "912aa43d87aff4c6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T10:49:49.256181Z",
     "start_time": "2024-09-20T10:49:49.246368Z"
    }
   },
   "cell_type": "code",
   "source": "print(p)",
   "id": "7c654ac20ad57d30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T10:49:49.268250Z",
     "start_time": "2024-09-20T10:49:49.256181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(W))  # len(p) = num_of_cohorts = 21 W is an array that each element represents a cohort with different p\n",
    "print(len(W[0]))  # M = 50 an element in W is an array of 50 weight matrices\n",
    "print(len(W[0][0][0]))  # N = 100 num of genes each weight matrix is an NxN matrix\n",
    "print(len(W[0][0][1]))  # N = 100 num of genes each weight matrix is an NxN matrix"
   ],
   "id": "b4c98166430d90c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "10\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T10:49:50.991634Z",
     "start_time": "2024-09-20T10:49:49.268250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for cohort_index in range(num_of_cohorts):\n",
    "    # for each cohort, create a system of 50 w defect matrices\n",
    "\n",
    "    # Set the base weight matrix for the current cohort\n",
    "    # TODO: this is weird since I defined avg_deg as the average degree but I use it as a probability distribution (actual average might be different) maybe change that in the future\n",
    "    base_model = np.array([[np.random.uniform(0, 2 * q) if (row != column and dist(avg_deg / (N - 1))) else 0\n",
    "                            for column in range(N)] for row in range(N)])\n",
    "\n",
    "    # TODO: the first iteration is useless since p[0] = 0 by definition and its a simple copy of base_model. Maybe change that in the future\n",
    "    # Create defects of the weight matrix\n",
    "    # TODO: Too many indentations, maybe refactor this code\n",
    "    for defect_index in range(M):\n",
    "        for row in range(N):\n",
    "            for column in range(N):\n",
    "                if base_model[row, column] != 0:\n",
    "                    if dist(p[cohort_index]):\n",
    "                        # TODO: What if, by chance np.random.uniform(0, 2 * q) is the same as base_model[row, column] or if it is 0? these are edge cases that are rare and therefore not handled\n",
    "                        W[cohort_index][defect_index][row, column] = np.random.uniform(0, 2 * q)\n",
    "                    else:\n",
    "                        W[cohort_index][defect_index][row, column] = base_model[row, column]\n",
    "                else:\n",
    "                    W[cohort_index][defect_index][row, column] = 0"
   ],
   "id": "eadb1b5d7c5e4111",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-20T10:49:50.993675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This block takes too long to run :(\n",
    "# Initialize the results array\n",
    "# results[cohort index][defect index][(row, column)] - rows represents timestamps, columns represent gene index\n",
    "results = [[np.zeros((num_of_time_stamps, N)) for _ in range(M)] for __ in range(num_of_cohorts)]\n",
    "\n",
    "# Solve the ODEs for all M weight matrix and corresponding initial conditions\n",
    "for cohort_index in range(num_of_cohorts):\n",
    "    for defect_index in range(M):\n",
    "        # odeint will return a vector (function) where the rows represent discrete time frames and the columns represent the values of x_i so this vector is x_i(t)\n",
    "\n",
    "        results[cohort_index][defect_index] = odeint(ode_system, initial_conditions[defect_index], t,\n",
    "                                                     args=(W[cohort_index][defect_index],))"
   ],
   "id": "cfce80aab9beb059",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 17/110 [05:02<27:36, 17.81s/it]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## just testing things here\n",
    "results0 = results[0][0][-1][\n",
    "           :]  # list of genes in a single cell (first group of cohorts, first cell, -1 means steadystate time, : means get all the genes in this cell\n",
    "results0\n",
    "results1 = results[0][1][-1][:]  # (first group of cohorts, second cell, steady state time, and all the genes)\n",
    "results2 = results[0][2][-1][:]\n",
    "# ......\n",
    "resultsMc = results[num_of_cohorts - 1][M - 1][-1][:]"
   ],
   "id": "6ab110bd647b1c20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transposed_steady_state = [np.zeros((M, N)) for _ in range(num_of_cohorts)]\n",
    "for cohort_j in range(num_of_cohorts):\n",
    "    cur_co = results[cohort_j]\n",
    "    for cell_index in range(M):\n",
    "        cur_cell = cur_co[cell_index]\n",
    "        transposed_steady_state[cohort_j][cell_index] = (cur_cell[-1])\n",
    "actual_steady_state = [np.transpose(transposed_steady_state[i]) for i in range(num_of_cohorts)]\n",
    "actual_steady_state[0]"
   ],
   "id": "ebca03d462c0ef00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate ~~Negative~~ Spearman Correlation",
   "id": "3522b44c061ca59d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spearman_steady_state = [spearmanr(actual_steady_state[i])[0] for i in range(len(actual_steady_state))]",
   "id": "c3d125bd10d9c4ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(np.shape(spearman_steady_state))\n",
    "spearman_steady_state[0]\n",
    "spearmanr(actual_steady_state[2])[0]  # [0] gives the Spearman correlation matrix, [1] will give the p value\n",
    "\n",
    "(spearmanr(actual_steady_state[0])[0])\n",
    "len(spearmanr(actual_steady_state[0])[0])\n"
   ],
   "id": "a5b6d93fce969449",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get the off diagonal elements and graph a distribution of them:\n",
    "off_diagonal_elements = [[] for _ in range(len(spearman_steady_state))]\n",
    "\n",
    "for mnm in range(len(spearman_steady_state)):\n",
    "    for row_index in range(len(spearmanr(actual_steady_state[0])[0])):\n",
    "        for column_index in range(len(spearmanr(actual_steady_state[0])[0])):\n",
    "            if column_index > row_index:\n",
    "                off_diagonal_elements[mnm].append(spearman_steady_state[mnm][row_index][column_index])"
   ],
   "id": "33cd63934fa66a51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the off-diagonal elements distribution\n",
    "for sss in range(len(off_diagonal_elements)):\n",
    "    plt.hist(off_diagonal_elements[sss], alpha=0.5, label='cohort p = {:.2f}'.format(p[sss]))\n",
    "    \n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Off-diagonal Elements Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "id": "67d41ca3969ad500",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use dimensionality reduction to reduce the spearman matrix by using PCA\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare the data\n",
    "# spearman_steady_state is a list of Spearman correlation matrices (matrix for each cohort)\n",
    "# Flatten the Spearman correlation matrices for PCA (e.g. [[1,2], [3,4]] --> [[1, 2, 3, 4]])\n",
    "# I think I can use instead * when creating spearman but whatever\n",
    "flattened_spearman = [matrix.flatten() for matrix in spearman_steady_state]\n",
    "\n",
    "# Scale the data - this will take care of the normalization step I mentioned above \n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(flattened_spearman)\n",
    "\n",
    "# Apply PCA - project the components to the two PCi's with the highest variation\n",
    "pca = PCA(n_components=2)  # You can change the number of components as needed\n",
    "pca_result = pca.fit_transform(standardized_data)\n",
    "\n",
    "# [variance of PC1, variance of PC2]\n",
    "print(\"Variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the PCA result [PC1, PC2]\n",
    "print(\"PCA result:\\n\", pca_result)"
   ],
   "id": "980ef897b8de0a50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Assuming spearman_steady_state is a list of Spearman correlation matrices\n",
    "# Flatten the Spearman correlation matrices to 2D arrays for PCA\n",
    "flattened_spearman = [matrix.flatten() for matrix in spearman_steady_state]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(flattened_spearman)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # You can change the number of components as needed\n",
    "pca_result = pca.fit_transform(standardized_data)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the PCA result\n",
    "print(\"PCA result:\\n\", pca_result)\n",
    "\n",
    "# Plot the PCA results with different colors for each point\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(p)))\n",
    "for i in range(len(p)):\n",
    "    plt.scatter(pca_result[i, 0], pca_result[i, 1], color=colors[i], label=f'p = {p[i]:.2f}')\n",
    "\n",
    "plt.title('PCA of Spearman Correlation Matrices')\n",
    "plt.xlabel('Principal Component 1 (accounts for {:.2f}% of the variation)'.format(pca.explained_variance_ratio_[0] * 100))\n",
    "plt.ylabel('Principal Component 2 (accounts for {:.2f}% of the variation)'.format(pca.explained_variance_ratio_[1] * 100))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "5786a727e679ee3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### PCA\n",
    "check out this video: https://youtu.be/FgakZw6K1QQ\n",
    "\n",
    "##### PC1\n",
    "- Calculate the average measurement for gene 1, 2, 3, ..., N - get the \"center\" of the data\n",
    "- Recalculate the data relative to center (the center is the origin).\n",
    "- Find the best fit linear line that passes through the origin for the data by choosing the line that maximizes the sum of the distances of the points projected by each one of the points on to the line - this line is PC1.\n",
    "- Normalize the line to unit length it is the eigen vector (also called singular vector) of PC1. the proportion of each gene are called \"loading scores\" (kind of like tan(theta) = y/x in 2d). The average of the sum of squared distances (of the points projected on to the PC1 line) is the eigen value for PC1. The square root of the sum of the squared distances is called the singular value for PC1\n",
    "\n",
    "##### PC2\n",
    "- The line that is perpendicular to PC1 (m_pc1 * m_pc2 = -1 for 2d, the number of PC's will be the MIN(variables (genes), samples(cells))) This line is the eigen vector for PC2. The loading scores are the projection of the normalized line on to the gene1 axis (x) and the projection of the normalized line on to the gene2 axis (y). The average of the sum of squared distances (of the points projected on to the PC2 line) is the eigen value for PC2.\n",
    "\n",
    "PC1 accounts for eigen_value_pc1/(eigen_value_pc1 + eigen_value_pc2) of the total variation\n",
    "PC2 accounts for eigen_value_pc2 / (eigen_value_pc1 + eigen_value_pc2) of the total variation\n",
    "\n",
    "We may have many PCi's, but after we calculated the principal components we simply project them to the 2 most dominant pc's (the ones with the highest eigen value i.e. highest variation) NOTE: this will only be good if they account for most of the variation.\n",
    "Use a scree plot for the PCi's"
   ],
   "id": "354bbd0a80e1de99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "# load dataset into Pandas DataFrame\n",
    "df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])"
   ],
   "id": "46eb64f33d610922",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# results[cohort_index][defect_index][final_time=-1, :]\n",
    "results[0][0]  #first cohort(p=0), first set of M(=50) w's - it is a vector where rows = time and columns = values; I want the last row:\n",
    "steady_state_test = results[0][0][-1]\n",
    "steady_state_test\n",
    "steady_state = np.zeros((num_of_cohorts, N))\n",
    "\n",
    "# for cohort_i in range(num_of_cohorts):\n",
    "#     for defects_group_i in range(M):\n",
    "#         steady_state[]\n",
    "steady_state\n",
    "# steady_state = [results[cohort_i][:][-1] for cohort_i in range(num_of_cohorts)] for gene in range(N)"
   ],
   "id": "87d00cedf7a223e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the results\n",
    "for cohort_index in range(num_of_cohorts):\n",
    "    for defect_index in range(M):\n",
    "        for gene in range(N):\n",
    "            plt.plot(t, results[cohort_index][defect_index][:, gene])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('x value')\n",
    "plt.title('Michaelis-Menten kinetics')\n",
    "plt.show()"
   ],
   "id": "e448adc2e13f3d53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ---------",
   "id": "5c9b0f93c359b538"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# initial state\n",
    "results[0][0, :]"
   ],
   "id": "e1085715946c36d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_state = [np.random.rand(N) for _ in range(N)]\n",
    "# final state\n",
    "for m in range(M):\n",
    "    final_state[m] = results[m][-1, :]"
   ],
   "id": "fb9362171baed38e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the variance of the final state\n",
    "final_state_variance = np.var(final_state)\n",
    "print(final_state_variance)"
   ],
   "id": "7f49274c7e8869f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# W2_variance = np.var(W_strong_influence)\n",
    "# print(W_strong_influence)"
   ],
   "id": "ba5fca2bd453048",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the final state vector to a csv file\n",
    "# np.savetxt('final_state.csv', final_state, delimiter=',')"
   ],
   "id": "1bfc9999174e3e02",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
